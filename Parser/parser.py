from datetime import datetime

import requests
from bs4 import BeautifulSoup
import json
import time
import json
import csv
from urllib.parse import urljoin

def get_categories_requests():
    headers = {
            'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36'
        }

    response = requests.get("https://chefport.ru/catalog", headers=headers)
    soup = BeautifulSoup(response.content, 'html.parser')

        # –ò—â–µ–º –Ω–∞–≤–∏–≥–∞—Ü–∏–æ–Ω–Ω–æ–µ –º–µ–Ω—é
    nav = soup.find('ul', class_='b-catalog-nav')

    categories = []
    if nav:
            # –ò–∑–≤–ª–µ–∫–∞–µ–º –≤—Å–µ —ç–ª–µ–º–µ–Ω—Ç—ã —Å–ø–∏—Å–∫–∞
        items = nav.find_all('li', class_='b-catalog-nav__item')

        for item in items:
            link = item.find('a', class_='b-catalog-nav__item-link')
            if link:
                name = link.get_text(strip=True)
                url = link.get('href')
                categories.append({
                        'name': name,
                        'url': url
                })

    return categories


def parse_products_from_category(base_url):
    """
    –ü–∞—Ä—Å–∏—Ç –≤—Å–µ —Ç–æ–≤–∞—Ä—ã –∏–∑ –∫–∞—Ç–µ–≥–æ—Ä–∏–∏ —Å —É—á–µ—Ç–æ–º –ø–∞–≥–∏–Ω–∞—Ü–∏–∏
    """
    all_products = []
    page = 1

    while True:
        # –§–æ—Ä–º–∏—Ä—É–µ–º URL —Å—Ç—Ä–∞–Ω–∏—Ü—ã (–º–æ–∂–µ—Ç –±—ã—Ç—å —Å –ø–∞—Ä–∞–º–µ—Ç—Ä–æ–º page –∏–ª–∏ –∏–∑–º–µ–Ω—è—Ç—å—Å—è –ø–æ-–¥—Ä—É–≥–æ–º—É)
        if page == 1:
            url = base_url
        else:
            url = f"{base_url}?page={page}"  # –∏–ª–∏ –¥—Ä—É–≥–æ–π —Ñ–æ—Ä–º–∞—Ç –ø–∞–≥–∏–Ω–∞—Ü–∏–∏
            # https: // chefport.ru / catalog / moreproduktyi?page = 2

        print(f"–ü–∞—Ä—Å–∏–º —Å—Ç—Ä–∞–Ω–∏—Ü—É {page}: {url}")
        print(url)

        headers = {
            'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36',
            'Accept': 'text/html,application/xhtml+xml,application/xml;q=0.9,image/webp,*/*;q=0.8',
            'Accept-Language': 'ru-RU,ru;q=0.8,en-US;q=0.5,en;q=0.3',
        }

        try:
            response = requests.get(url, headers=headers)
            response.raise_for_status()

            soup = BeautifulSoup(response.content, 'html.parser')

            # –ü–∞—Ä—Å–∏–º —Ç–æ–≤–∞—Ä—ã —Å —Ç–µ–∫—É—â–µ–π —Å—Ç—Ä–∞–Ω–∏—Ü—ã
            products = parse_products_from_page(soup, base_url)

            if not products:
                print("–¢–æ–≤–∞—Ä—ã –Ω–µ –Ω–∞–π–¥–µ–Ω—ã, –∑–∞–≤–µ—Ä—à–∞–µ–º –ø–∞—Ä—Å–∏–Ω–≥")
                break

            all_products.extend(products)
            print(f"–ù–∞–π–¥–µ–Ω–æ —Ç–æ–≤–∞—Ä–æ–≤ –Ω–∞ —Å—Ç—Ä–∞–Ω–∏—Ü–µ: {len(products)}")

            # –ü—Ä–æ–≤–µ—Ä—è–µ–º –Ω–∞–ª–∏—á–∏–µ —Å–ª–µ–¥—É—é—â–µ–π —Å—Ç—Ä–∞–Ω–∏—Ü—ã
            # if not has_next_page(soup):
            #     print("–≠—Ç–æ –ø–æ—Å–ª–µ–¥–Ω—è—è —Å—Ç—Ä–∞–Ω–∏—Ü–∞")
            #     break

            page += 1
            time.sleep(1)  # –ó–∞–¥–µ—Ä–∂–∫–∞ –º–µ–∂–¥—É –∑–∞–ø—Ä–æ—Å–∞–º–∏

        except requests.RequestException as e:
            print(f"–û—à–∏–±–∫–∞ –ø—Ä–∏ –∑–∞–ø—Ä–æ—Å–µ —Å—Ç—Ä–∞–Ω–∏—Ü—ã {page}: {e}")
            break

    return all_products


def parse_products_from_page(soup, base_url):
    """
    –ü–∞—Ä—Å–∏—Ç —Ç–æ–≤–∞—Ä—ã —Å –æ–¥–Ω–æ–π —Å—Ç—Ä–∞–Ω–∏—Ü—ã
    """
    products = []

    # –ò—â–µ–º –≤—Å–µ –∫–∞—Ä—Ç–æ—á–∫–∏ —Ç–æ–≤–∞—Ä–æ–≤
    product_items = soup.find_all('div', class_='b-product-item')

    for item in product_items:
        try:
            product_data = {}

            # –ù–∞–∑–≤–∞–Ω–∏–µ —Ç–æ–≤–∞—Ä–∞
            title_element = item.find('h3', class_='b-product-item__information-title')
            if title_element:
                product_data['name'] = title_element.get_text(strip=True)
            else:
                product_data['name'] = None

            # –°—Å—ã–ª–∫–∞ –Ω–∞ —Ç–æ–≤–∞—Ä
            link_element = item.find('a', class_='b-product-item__image')
            if link_element and link_element.get('href'):
                product_data['url'] = urljoin(base_url, link_element.get('href'))
            else:
                # –ü—Ä–æ–±—É–µ–º –Ω–∞–π—Ç–∏ —Å—Å—ã–ª–∫—É –≤ –∑–∞–≥–æ–ª–æ–≤–∫–µ
                title_link = item.find('a', href=True)
                if title_link:
                    product_data['url'] = urljoin(base_url, title_link.get('href'))
                else:
                    product_data['url'] = None

            # –ò–∑–æ–±—Ä–∞–∂–µ–Ω–∏–µ
            img_element = item.find('img', class_='img-fluid')
            if img_element and img_element.get('src'):
                product_data['image'] = urljoin(base_url, img_element.get('src'))
                product_data['image_alt'] = img_element.get('alt', '')
            else:
                product_data['image'] = None
                product_data['image_alt'] = ''

            # –¶–µ–Ω–∞
            price_element = item.find('div', class_='b-product-item-price__current')
            if price_element:
                price_text = price_element.get_text(strip=True)
                # –ò–∑–≤–ª–µ–∫–∞–µ–º —Ç–æ–ª—å–∫–æ —Ü–∏—Ñ—Ä—ã –∏–∑ —Ü–µ–Ω—ã
                product_data['price'] = extract_price(price_text)
            else:
                product_data['price'] = None

            # –ï–¥–∏–Ω–∏—Ü–∞ –∏–∑–º–µ—Ä–µ–Ω–∏—è
            unit_element = item.find('div', class_='b-product-item-price__unit')
            if unit_element:
                product_data['unit'] = unit_element.get_text(strip=True).replace('–∑–∞\xa0', '')
            else:
                product_data['unit'] = '—à—Ç.'  # –∑–Ω–∞—á–µ–Ω–∏–µ –ø–æ —É–º–æ–ª—á–∞–Ω–∏—é

            # –î–æ–ø–æ–ª–Ω–∏—Ç–µ–ª—å–Ω–∞—è –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏—è
            product_id_element = item.find('input', {'name': 'product_id'})
            if product_id_element:
                product_data['product_id'] = product_id_element.get('value')
            else:
                product_data['product_id'] = None

            # –û–±—â–∞—è —Å—Ç–æ–∏–º–æ—Å—Ç—å (–µ—Å–ª–∏ –µ—Å—Ç—å)
            total_price_element = item.find('span', class_='totalPrice')
            if total_price_element:
                product_data['total_price'] = total_price_element.get_text(strip=True)
            else:
                product_data['total_price'] = None

            products.append(product_data)

        except Exception as e:
            print(f"–û—à–∏–±–∫–∞ –ø—Ä–∏ –ø–∞—Ä—Å–∏–Ω–≥–µ —Ç–æ–≤–∞—Ä–∞: {e}")
            continue

    return products


def extract_price(price_text):
    """
    –ò–∑–≤–ª–µ–∫–∞–µ—Ç —á–∏—Å–ª–æ–≤–æ–µ –∑–Ω–∞—á–µ–Ω–∏–µ —Ü–µ–Ω—ã –∏–∑ —Ç–µ–∫—Å—Ç–∞
    """
    try:
        # –£–±–∏—Ä–∞–µ–º –≤—Å–µ –Ω–µ—Ü–∏—Ñ—Ä–æ–≤—ã–µ —Å–∏–º–≤–æ–ª—ã, –∫—Ä–æ–º–µ —Ç–æ—á–µ–∫ –∏ –∑–∞–ø—è—Ç—ã—Ö
        cleaned = ''.join(c for c in price_text if c.isdigit() or c in ',.')
        # –ó–∞–º–µ–Ω—è–µ–º –∑–∞–ø—è—Ç—É—é –Ω–∞ —Ç–æ—á–∫—É –¥–ª—è –ø—Ä–µ–æ–±—Ä–∞–∑–æ–≤–∞–Ω–∏—è –≤ float
        cleaned = cleaned.replace(',', '.').replace(' ', '')
        return float(cleaned) if cleaned else None
    except (ValueError, TypeError):
        return None


def has_next_page(soup):
    """
    –ü—Ä–æ–≤–µ—Ä—è–µ—Ç –Ω–∞–ª–∏—á–∏–µ —Å–ª–µ–¥—É—é—â–µ–π —Å—Ç—Ä–∞–Ω–∏—Ü—ã
    """
    # –ò—â–µ–º —ç–ª–µ–º–µ–Ω—Ç—ã –ø–∞–≥–∏–Ω–∞—Ü–∏–∏
    next_button = soup.find('a', string=['>', '–î–∞–ª–µ–µ', 'Next'])
    if next_button and next_button.get('href'):
        return True

    # –ò—â–µ–º –∞–∫—Ç–∏–≤–Ω—É—é —Å—Ç—Ä–∞–Ω–∏—Ü—É –∏ —Å–ª–µ–¥—É—é—â—É—é –∑–∞ –Ω–µ–π
    current_page = soup.find('li', class_=lambda x: x and 'active' in x)
    if current_page:
        next_page = current_page.find_next_sibling('li')
        if next_page and next_page.find('a'):
            return True

    # –ü—Ä–æ–≤–µ—Ä—è–µ–º –Ω–∞–ª–∏—á–∏–µ –ø–∞–≥–∏–Ω–∞—Ü–∏–æ–Ω–Ω–æ–≥–æ –∫–æ–Ω—Ç–µ–π–Ω–µ—Ä–∞
    pagination = soup.find('ul', class_=lambda x: x and 'pagination' in x.lower())
    if pagination:
        pages = pagination.find_all('a')
        return len(pages) > 1

    return False


def save_products_to_csv(products, filename='oysters_products.csv'):
    """
    –°–æ—Ö—Ä–∞–Ω—è–µ—Ç —Å–ø–∏—Å–æ–∫ —Ç–æ–≤–∞—Ä–æ–≤ –≤ CSV —Ñ–∞–π–ª

    Args:
        products (list): –°–ø–∏—Å–æ–∫ —Å–ª–æ–≤–∞—Ä–µ–π —Å –¥–∞–Ω–Ω—ã–º–∏ —Ç–æ–≤–∞—Ä–æ–≤
        filename (str): –ù–∞–∑–≤–∞–Ω–∏–µ —Ñ–∞–π–ª–∞ –¥–ª—è —Å–æ—Ö—Ä–∞–Ω–µ–Ω–∏—è
    """
    if not products:
        print("–ù–µ—Ç –¥–∞–Ω–Ω—ã—Ö –¥–ª—è —Å–æ—Ö—Ä–∞–Ω–µ–Ω–∏—è")
        return

    # –û–ø—Ä–µ–¥–µ–ª—è–µ–º –≤—Å–µ –≤–æ–∑–º–æ–∂–Ω—ã–µ –ø–æ–ª—è –∏–∑ –¥–∞–Ω–Ω—ã—Ö
    fieldnames = set()
    for product in products:
        fieldnames.update(product.keys())

    # –ü—Ä–µ–æ–±—Ä–∞–∑—É–µ–º –≤ —Å–ø–∏—Å–æ–∫ –¥–ª—è —Å–æ—Ö—Ä–∞–Ω–µ–Ω–∏—è –ø–æ—Ä—è–¥–∫–∞
    fieldnames = list(fieldnames)

    with open(filename, 'w', newline='', encoding='utf-8') as file:
        writer = csv.DictWriter(file, fieldnames=fieldnames)
        writer.writeheader()

        for product in products:
            writer.writerow(product)

    print(f"–°–æ—Ö—Ä–∞–Ω–µ–Ω–æ {len(products)} —Ç–æ–≤–∞—Ä–æ–≤ –≤ —Ñ–∞–π–ª {filename}")


def save_products_to_json(products, filename='moreproduktyi_products.json'):
    """
    –°–æ—Ö—Ä–∞–Ω—è–µ—Ç —Ç–æ–≤–∞—Ä—ã –≤ JSON —Ñ–∞–π–ª
    """
    with open(filename, 'w', encoding='utf-8') as file:
        json.dump(products, file, ensure_ascii=False, indent=2)

    print(f"–°–æ—Ö—Ä–∞–Ω–µ–Ω–æ {len(products)} —Ç–æ–≤–∞—Ä–æ–≤ –≤ —Ñ–∞–π–ª {filename}")


def create_complete_catalog_json(categories, products):
    """–°–æ–∑–¥–∞–µ—Ç –ø–æ–ª–Ω—ã–π JSON –∫–∞—Ç–∞–ª–æ–≥"""

    # –î–æ–±–∞–≤–ª—è–µ–º —Å—á–µ—Ç—á–∏–∫–∏ —Ç–æ–≤–∞—Ä–æ–≤ –≤ –∫–∞—Ç–µ–≥–æ—Ä–∏–∏
    for category in categories:
        category_products = [p for p in products if p.get('url') == category['url']]
        category['product_count'] = len(category_products)

    catalog = {
        "metadata": {
            "source": "chefport.ru",
            "total_categories": len(categories),
            "total_products": len(products),
            "last_updated": datetime.now().isoformat(),
            "version": "1.0"
        },
        "categories": categories,
        "products": products
    }

    return catalog


def save_catalog_to_json(catalog, filename=None):
    """–°–æ—Ö—Ä–∞–Ω—è–µ—Ç –∫–∞—Ç–∞–ª–æ–≥ –≤ JSON —Ñ–∞–π–ª"""
    if filename is None:
        timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
        filename = f"chefport_catalog_{timestamp}.json"

    with open(filename, 'w', encoding='utf-8') as f:
        json.dump(catalog, f, ensure_ascii=False, indent=2)

    print(f"üíæ –ö–∞—Ç–∞–ª–æ–≥ —Å–æ—Ö—Ä–∞–Ω–µ–Ω –≤ —Ñ–∞–π–ª: {filename}")
    return filename